{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNZIP FILES IN TRAIN AND LABEL FOLDERS \n",
    "# COPY SPECIFIC FILES TO TRAIN AND LABEL FOLDERS\n",
    "# FILES: .obj, .json \n",
    "# RAW DATA FOLDER: test_raw\n",
    "# UNZIPPED DATA FOLDER: test_uzip\n",
    "# SUBFOLDERS: train, label\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "log_print = False\n",
    "root_path = os.path.abspath(os.getcwd())\n",
    "zip_path = os.path.join(root_path, \"test_raw\")\n",
    "data_path = os.path.join(root_path, \"test_uzip\")\n",
    "# train_data_path = os.path.join(data_path, \"train\")\n",
    "tmp_path = os.path.join(zip_path, \"tmp\")\n",
    "# os.makedirs(train_data_path, exist_ok=True)\n",
    "\n",
    "# Check if there are any folders in test_raw\n",
    "if not os.listdir(zip_path):\n",
    "    log.info(\"No folders found in %s\", zip_path)\n",
    "else:\n",
    "    for folder in os.listdir(zip_path):\n",
    "        if 'train' in folder:\n",
    "            current_train_folder = os.path.join(zip_path, folder)\n",
    "            log.info(f\"current_train_folder: {current_train_folder}\")\n",
    "            log.info(f\"number of zip files: {len(os.listdir(current_train_folder))}\")\n",
    "            \n",
    "            # Check if there are any files in the current_train_folder\n",
    "            if not os.listdir(current_train_folder):\n",
    "                log.info(f\"No files found in {current_train_folder}\")\n",
    "            else:\n",
    "                for file in os.listdir(current_train_folder):\n",
    "                    if 'zip' in file:\n",
    "                        zip_tmp = os.path.join(current_train_folder, file)\n",
    "                        train_data_path = os.path.join(data_path, folder)\n",
    "                        os.makedirs(train_data_path, exist_ok=True)\n",
    "                        if log_print: log.info(f\"file to extract: {zip_tmp}\")\n",
    "                        if log_print: log.info(f\"file to folder: {train_data_path}\")\n",
    "                        \n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_tmp, 'r') as zip_ref:\n",
    "                                zip_ref.extractall(tmp_path)\n",
    "                                for file in os.listdir(tmp_path):\n",
    "                                    if 'obj' in file:\n",
    "                                        file_tocp = os.path.join(tmp_path, file)\n",
    "                                        shutil.copy(file_tocp, train_data_path)\n",
    "                                    os.remove(tmp_path + '/' + file)\n",
    "                            if log_print: log.info(f\"Successfully unzipped {file_tocp} to {train_data_path}\")\n",
    "                        except Exception as e:\n",
    "                            log.error(f\"Error unzipping {file_tocp}: {e}\")\n",
    "                log.info(f\"number of copied {folder} files: {len(os.listdir(train_data_path))}\")\n",
    "                \n",
    "        elif 'label' in folder: \n",
    "            current_label_folder = os.path.join(zip_path, folder)\n",
    "            log.info(f\"current_label_folder: {current_label_folder}\")\n",
    "            log.info(f\"number of zip files: {len(os.listdir(current_label_folder))}\")\n",
    "            if not os.listdir(current_label_folder):\n",
    "                log.info(f\"No files found in {current_label_folder}\")\n",
    "            else:\n",
    "                for file in os.listdir(current_label_folder):\n",
    "                    if 'zip' in file and '_2D' not in file and 'META' not in file:\n",
    "                        zip_tmp = os.path.join(current_label_folder, file)\n",
    "                        label_data_path= os.path.join(data_path, folder)\n",
    "                        os.makedirs(label_data_path, exist_ok=True)\n",
    "                        if log_print: log.info(f\"file to extract: {zip_tmp}\")\n",
    "                        if log_print: log.info(f\"file to folder: {label_data_path}\")\n",
    "                        \n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_tmp, 'r') as zip_ref:\n",
    "                                zip_ref.extractall(tmp_path)\n",
    "                                for file in os.listdir(tmp_path):\n",
    "                                    if 'mpie68.json' in file:\n",
    "                                        file_tocp = os.path.join(tmp_path, file)\n",
    "                                        shutil.copy(file_tocp, label_data_path)\n",
    "                                    os.remove(tmp_path + '/' + file)\n",
    "                            if log_print: log.info(f\"Successfully unzipped {file_tocp} to {label_data_path}\")\n",
    "                        except Exception as e:\n",
    "                            log.error(f\"Error unzipping {file_tocp}: {e}\")\n",
    "                log.info(f\"number of copied {folder} files: {len(os.listdir(label_data_path))}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXTRA FUNCTIONS FOR DATA LOCATION PREPARATION\n",
    "def print_max_lnd(data):\n",
    "    landmarks = data[\"landmarks\"]\n",
    "    \n",
    "    x_values = [point[\"x\"] for point in landmarks]\n",
    "    y_values = [point[\"y\"] for point in landmarks]\n",
    "    z_values = [point[\"z\"] for point in landmarks]\n",
    "\n",
    "    x_range = (min(x_values), max(x_values))\n",
    "    y_range = (min(y_values), max(y_values))\n",
    "    z_range = (min(z_values), max(z_values))\n",
    "\n",
    "    print(f\"X range: {x_range}\")\n",
    "    print(f\"Y range: {y_range}\")\n",
    "    print(f\"Z range: {z_range}\")\n",
    "    \n",
    "def normalize_points(points):\n",
    "    \"\"\"Normalize a list of points (x, y, z) to have a mean of 0 and a standard deviation of 1\"\"\"\n",
    "    points_np = np.array(points)\n",
    "    mean = np.mean(points_np, axis=0)\n",
    "    std_dev = np.std(points_np, axis=0)\n",
    "    normalized_points = (points_np - mean) / std_dev\n",
    "    return normalized_points.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:number of converted 1107M_FC_O-mpie68.json files: 54\n"
     ]
    }
   ],
   "source": [
    "# REED COCO FORMAT 3D LANDMARKS FROM JSON FILE IN LABEL FOLDER\n",
    "# CONVERT LANDMARKS TO ARRAY\n",
    "# SAVE 3D ARRAY TO NPY FILE\n",
    "# FILES:  .json, .npy\n",
    "# RAW DATA FOLDER: test_unzip/label\n",
    "# CONVERTED DATA FOLDER: test_unzip/label (the same folder)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "root_path = os.path.abspath(os.getcwd())\n",
    "data_path = os.path.join(root_path, \"test_uzip\")\n",
    "json_path = os.path.join(data_path, \"label\")\n",
    "log_print = False\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARNING)\n",
    "\n",
    "# Check if there are any folders in test_raw\n",
    "if not os.listdir(json_path):\n",
    "    log.info(\"No folders found in %s\", json_path)\n",
    "else:\n",
    "    for json_file in os.listdir(json_path):\n",
    "        if 'mpie68.json' in json_file:\n",
    "            json_filepath = os.path.join(json_path, json_file)\n",
    "            \n",
    "            # Open and read the JSON file\n",
    "            with open(json_filepath, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # show max and min values of landmarks\n",
    "                # print_max_lnd(data)\n",
    "                \n",
    "                # Extract landmarks\n",
    "                landmarks_data = data.get('landmarks', [])\n",
    "                \n",
    "                # Convert landmarks to a list of [x, y, z] and then to a numpy array\n",
    "                landmarks_list = [[point['x'], point['y'], point['z']] for point in landmarks_data]\n",
    "                landmarks_array = np.array(landmarks_list)\n",
    "                \n",
    "                # normalize landmarks\n",
    "                # landmarks_array = normalize_points(landmarks_array)\n",
    "                \n",
    "                # Save the numpy array to an .npy file\n",
    "                npy_filepath = os.path.join(json_path, json_file.replace('.json', '.npy'))\n",
    "                np.save(npy_filepath, landmarks_array)\n",
    "                \n",
    "                if log_print:\n",
    "                    log.info(f\"Saved landmarks from {json_file} to {npy_filepath}\")\n",
    "    log.info(f\"number of converted {json_file} files: {len(os.listdir(json_path))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|------------- obj_n: /home/jakhon37/myprojects/3DFACE/mica_DEV/MICA/dataset/example/out_faceE.obj\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root = os.path.abspath(os.getcwd())\n",
    "obj_new = os.path.join(root, \"example/out_faceE.obj\")\n",
    "print('\\n\\n|------------- obj_n:', obj_new)\n",
    "\n",
    "obj_in = os.path.join(root, \"test_uzip/train/1107M_FC_A.obj\")\n",
    "\n",
    "# Read the file\n",
    "with open(obj_in, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "inside_mouth_socket = False\n",
    "deleted_vertex_indices = set()\n",
    "vertex_count = 0\n",
    "filtered_lines = []\n",
    "\n",
    "for line in lines:\n",
    "    if \"g mouth_socket\" in line:\n",
    "        inside_mouth_socket = True\n",
    "        continue\n",
    "    if \"g \" in line and \"mouth_socket\" not in line:\n",
    "        inside_mouth_socket = False\n",
    "    if line.startswith(\"v \"):\n",
    "        vertex_count += 1\n",
    "        if inside_mouth_socket:\n",
    "            deleted_vertex_indices.add(vertex_count)\n",
    "            continue\n",
    "    if not inside_mouth_socket:\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "# Adjust face indices\n",
    "adjusted_lines = []\n",
    "for line in filtered_lines:\n",
    "    if line.startswith(\"f \"):\n",
    "        parts = line.split()\n",
    "        face_vertices = [int(part.split('/')[0]) for part in parts[1:]]\n",
    "        if any(idx in deleted_vertex_indices for idx in face_vertices):\n",
    "            continue  # Skip faces that reference deleted vertices\n",
    "        adjusted_parts = [\"f\"]\n",
    "        for part in parts[1:]:\n",
    "            vertex_index = int(part.split('/')[0])\n",
    "            adjusted_vertex_index = vertex_index - len([idx for idx in deleted_vertex_indices if idx < vertex_index])\n",
    "            adjusted_parts.append(part.replace(str(vertex_index), str(adjusted_vertex_index)))\n",
    "        adjusted_lines.append(' '.join(adjusted_parts) + '\\n')\n",
    "    else:\n",
    "        adjusted_lines.append(line)\n",
    "\n",
    "# Write the adjusted lines back to the file\n",
    "with open(obj_new, 'w') as file:\n",
    "    file.writelines(adjusted_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|------------- obj_n: /home/jakhon37/myprojects/3DFACE/mica_DEV/MICA/dataset/example/hello_flame.obj\n",
      "\n",
      "\n",
      "|------------- obj_n: /home/jakhon37/myprojects/3DFACE/mica_DEV/MICA/dataset/example/out_face.obj\n",
      "\n",
      "len of vertices: 5023\n",
      "len of faces: 9976\n",
      "len of textures: 0\n",
      "\n",
      "len of vertices: 19275\n",
      "len of faces: 16858\n",
      "len of textures: 19500\n",
      "\n",
      "\n",
      "|------------- landmarks3d: 68\n",
      "\n",
      "\n",
      "|------------- vertices_n: 5023\n",
      "|------------- vertices_: 19275\n",
      "|------------- faces_n: 9976\n",
      "|------------- faces_: 16858\n",
      "\n",
      "\n",
      "|------------- mean2: [-0.00563593 -0.00093774  0.00206536]\n",
      "|------------- range2: [0.229568 0.332232 0.236763]\n",
      "len of transformed_vertices_: 19275\n",
      "len of faces_: 16858\n",
      "len of transformed_3dlnd: 51\n"
     ]
    }
   ],
   "source": [
    "# REED 2 OBJ FACE MESHES \n",
    "# 1ST OBJ FILE IS UNNORMALIZED\n",
    "# 2ND OBJ FILE IS NORMALIZED\n",
    "# NEED TO NORMALIZE 1ST OBJ FILE BAED ON 2ND OBJ FILE\n",
    "\n",
    "import util\n",
    "import os \n",
    "import numpy as np\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "def load_obj(obj_path):\n",
    "    vertices, textures, faces = [], [], []\n",
    "    with open(obj_path, 'r') as fp:\n",
    "        for line in fp:\n",
    "            line_split = line.split()\n",
    "            if not line_split:\n",
    "                continue\n",
    "            elif line_split[0] == 'v':\n",
    "                vertices.append(list(map(float, line_split[1:4])))\n",
    "            elif line_split[0] == 'vt':\n",
    "                textures.append(list(map(float, line_split[1:3])))\n",
    "            elif line_split[0] == 'f':\n",
    "                face = [list(map(int, vert.split('/'))) for vert in line_split[1:]]\n",
    "                faces.append(face)\n",
    "    # return np.array(vertices), np.array(textures), faces\n",
    "    vertices, textures, faces = np.array(vertices), np.array(textures), faces\n",
    "    print('\\nlen of vertices:', len(vertices))\n",
    "    print('len of faces:', len(faces))\n",
    "    print('len of textures:', len(textures))\n",
    "    return vertices, textures, faces\n",
    "\n",
    "\n",
    "\n",
    "def normalize_obj_and_landmarks(obj_vert, obj_n_vert, landmarks):\n",
    "    # Convert to numpy arrays\n",
    "    vertices1_np = np.array(obj_vert)\n",
    "    vertices2_np = np.array(obj_n_vert)\n",
    "    landmarks_np = np.array(landmarks)\n",
    "\n",
    "    # Calculate mean and range for the unnormalized vertices\n",
    "    mean1 = np.mean(vertices1_np, axis=0)\n",
    "    range1 = np.max(vertices1_np, axis=0) - np.min(vertices1_np, axis=0)\n",
    "\n",
    "    # Normalize the unnormalized vertices\n",
    "    normalized_vertices1 = (vertices1_np - mean1) / range1\n",
    "\n",
    "    # Normalize the landmarks using the same mean and range\n",
    "    normalized_landmarks = (landmarks_np - mean1) / range1\n",
    "\n",
    "    # Calculate mean and range for the normalized vertices\n",
    "    mean2 = np.mean(vertices2_np, axis=0)\n",
    "    range2 = np.max(vertices2_np, axis=0) - np.min(vertices2_np, axis=0)\n",
    "    print('\\n\\n|------------- mean2:', mean2)\n",
    "    print('|------------- range2:', range2)\n",
    "\n",
    "    # Scale and translate the normalized vertices to match the second set\n",
    "    transformed_vertices1 = normalized_vertices1 * range2 + mean2\n",
    "\n",
    "    # Scale and translate the normalized landmarks to match the second set\n",
    "    transformed_landmarks = normalized_landmarks * range2 + mean2\n",
    "\n",
    "    return transformed_vertices1.tolist(), transformed_landmarks.tolist()\n",
    "\n",
    "\n",
    "# 0-16 - jaw points \n",
    "# 17-26  - eye brow points \n",
    "# 27-35 - nose points \n",
    "# 36-47 -  eye points \n",
    "# 48-67 -  mouth points \n",
    "\n",
    "# for 68 face points mapping \n",
    "\n",
    "\n",
    "# eye brow: 1-10\n",
    "# nose: 11-19\n",
    "# eye: 20-31\n",
    "# outer mouth: 32-43\n",
    "# inner mouth: 44-51 \n",
    "\n",
    "# for flame 51 face points mapping\n",
    "\n",
    "def map_68_to_51(landmarks_68):\n",
    "    # A potential mapping from 68-point to 51-point landmarks\n",
    "    # This is a general mapping and might need adjustments based on the specific FLAME configuration\n",
    "    flame_mapping = [\n",
    "        # Eyebrow (5 points from each eyebrow)\n",
    "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
    "        # Nose\n",
    "        27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    "        # Eyes (6 points from each eye)\n",
    "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
    "        # Outer lip\n",
    "        48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
    "        # Inner lip\n",
    "        60, 61, 62, 63, 64, 63, 62, 61\n",
    "        # 60, 61, 62, 63, 64, 65, 66, 67\n",
    "    ]\n",
    "\n",
    "    # Extract the 51-point landmarks from the 68-point set\n",
    "    landmarks_51 = [landmarks_68[i] for i in flame_mapping]\n",
    "    return landmarks_51\n",
    "# eye brow: 1-10\n",
    "# nose: 11-19\n",
    "# eye: 20-31\n",
    "# outer mouth: 32-43\n",
    "# inner mouth: 44-51        \n",
    "\n",
    "\n",
    "def write_obj(filename, vertices, faces=None):\n",
    "    \"\"\"\n",
    "    Write vertices and faces to an OBJ file.\n",
    "\n",
    "    :param filename: Name of the OBJ file to write to.\n",
    "    :param vertices: List of vertices. Each vertex is a tuple of (x, y, z) coordinates.\n",
    "    :param faces: List of faces. Each face is a tuple of vertex indices. (Optional)\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        # Write vertices to the file\n",
    "        for vertex in vertices:\n",
    "            f.write(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
    "\n",
    "        # If faces are provided, write them to the file\n",
    "        if faces:\n",
    "            # for face in faces:\n",
    "            #     f.write(f\"f {' '.join(map(str, face))}\\n\")\n",
    "\n",
    "                # Write faces to the file\n",
    "            for face in faces:\n",
    "                face_str = ' '.join([f\"{idx[0]}/{idx[1]}\" for idx in face])\n",
    "                f.write(f\"f {face_str}\\n\")\n",
    "            \n",
    "root = os.path.abspath(os.getcwd())\n",
    "obj_n = os.path.join(root, \"example/hello_flame.obj\")\n",
    "print('\\n\\n|------------- obj_n:', obj_n)\n",
    "\n",
    "obj_ = os.path.join(root, \"test_uzip/train/1107M_FC_A.obj\")\n",
    "obj_ = os.path.join(root, \"test_uzip/train/1125M_FC_A.obj\")\n",
    "obj_ = os.path.join(root, \"example/1107M_FC_A.obj\")\n",
    "obj_ = os.path.join(root, \"example/out_face.obj\")\n",
    "\n",
    "lnd3d = os.path.join(root, \"test_uzip/label/1107M_FC_A-mpie68.npy\")\n",
    "# lnd3d = os.path.join(root, \"test_uzip/label/1125M_FC_A-mpie68.npy\")\n",
    "print('\\n\\n|------------- obj_n:', obj_)\n",
    "\n",
    "nrm_obj = os.path.join(root, \"example/nrm_n07.obj\")\n",
    "nrm3dlnd = os.path.join(root, \"example/nrm_n-mpie68_07.npy\")\n",
    "\n",
    "\n",
    "vertices_n, textures_n, faces_n = load_obj(obj_n)\n",
    "vertices_, textures_, faces_ = load_obj(obj_)\n",
    "landmarks3d = np.load(lnd3d)\n",
    "print('\\n\\n|------------- landmarks3d:', len(landmarks3d))\n",
    "print('\\n\\n|------------- vertices_n:', len(vertices_n))\n",
    "print('|------------- vertices_:', len(vertices_))\n",
    "print('|------------- faces_n:', len(faces_n))\n",
    "print('|------------- faces_:', len(faces_))\n",
    "\n",
    "transformed_vertices_, tformed_3dlnd = normalize_obj_and_landmarks(vertices_, vertices_n, landmarks3d)\n",
    "\n",
    "# tformed_3dlnd = [[round(val, 7) for val in row] for row in tformed_3dlnd]\n",
    "transformed_3dlnd_51 = map_68_to_51(tformed_3dlnd)\n",
    "# for row in transformed_3dlnd:\n",
    "#     print(f\"[ {row[0]: <10} {row[1]: <10} {row[2]: <10} ]\")\n",
    "# print('\\n\\n|------------- transformed_vertices_:', transformed_3dlnd_51)\n",
    "    \n",
    "# transformed_vertices_ = normlize_obj(vertices_n, vertices_)\n",
    "# transformed_vertices_ = transformed_vertices_.tolist()\n",
    "print('len of transformed_vertices_:', len(transformed_vertices_))\n",
    "print('len of faces_:', len(faces_))\n",
    "print('len of transformed_3dlnd:', len(transformed_3dlnd_51))\n",
    "\n",
    "write_obj(nrm_obj, transformed_vertices_, faces_)\n",
    "np.save(nrm3dlnd, transformed_3dlnd_51) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|------------- obj_out: /home/jakhon37/myprojects/3DFACE/mica_DEV/MICA/dataset/example/nrm_n07_scaled.obj\n"
     ]
    }
   ],
   "source": [
    "# ADJUST FACES AND VERTICES IN OBJ FILE\n",
    "# RESCALE TO FLAME SCALE\n",
    "# FILES: .obj\n",
    "\n",
    "# ! pip install open3d\n",
    "import os \n",
    "import trimesh\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_face_to_mesh(mesh):\n",
    "    # Get the last face\n",
    "    last_face = mesh.faces[-1]\n",
    "    \n",
    "    # Duplicate the last face\n",
    "    new_face = last_face.copy()\n",
    "    \n",
    "    # Add the new face to the mesh\n",
    "    mesh.faces = np.vstack([mesh.faces, new_face])\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "\n",
    "def rescale_tofalme(inObjPath, outObjPath, ad_vert=False,  add_face=False):\n",
    "    # Load the original mesh\n",
    "    mesh = trimesh.load_mesh(inObjPath)\n",
    "    \n",
    "    # Decimate the mesh\n",
    "    decimated_mesh = mesh.simplify_quadratic_decimation(face_count=9950)  # target number of faces 9552\n",
    "    \n",
    "    # Add an extra face if desired faces has not been reached\n",
    "    if add_face:\n",
    "        decimated_mesh = add_face_to_mesh(decimated_mesh)\n",
    "    \n",
    "    # Save the decimated mesh\n",
    "    decimated_mesh.export(outObjPath)\n",
    "    \n",
    "    if ad_vert:\n",
    "        # Duplicate the last vertex\n",
    "        new_vertex = decimated_mesh.vertices[-1]\n",
    "        # Add the new vertex to the vertices array\n",
    "        decimated_mesh.vertices = np.vstack([decimated_mesh.vertices, new_vertex])\n",
    "        # Save the modified mesh\n",
    "        decimated_mesh.export(outObjPath)\n",
    "\n",
    "\n",
    "root = os.path.abspath(os.getcwd())\n",
    "obj_in = os.path.join(root, \"example/nrm_n07.obj\")\n",
    "# obj_in = os.path.join(root, \"example/fitting_exmpl.obj\")\n",
    "obj_out = os.path.join(root, \"example/nrm_n07_scaled.obj\")\n",
    "# obj_out = os.path.join(root, \"example/fitting_ex.obj\")\n",
    "\n",
    "rescale_tofalme(obj_in, obj_out,  ad_vert=False, add_face=False)\n",
    "print('\\n\\n|------------- obj_out:', obj_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|------------- obj_out: /home/jakhon37/myprojects/3DFACE/mica_DEV/MICA/dataset/example/nrm_n07_scaled.ply\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# CONVERT OBJ TO PLY USING PYVISTA\n",
    "# FILES: .obj, .ply\n",
    "\n",
    "import pyvista as pv\n",
    "import pyvista as pv\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "\n",
    "root = os.path.abspath(os.getcwd())\n",
    "obj_in = os.path.join(root, \"example/nrm_n07_scaled.obj\")\n",
    "obj_in = os.path.join(root, \"example/nrm_n07.obj\")\n",
    "# obj_in = os.path.join(root, \"example/nrm_n07.obj\")\n",
    "obj_out = os.path.join(root, \"example/nrm_n07_scaled.ply\")\n",
    "# rescale_tofalme2(obj_in, obj_out, ad_vert=True)\n",
    "print('\\n\\n|------------- obj_out:', obj_out)\n",
    "# Load the original mesh\n",
    "mesh = pv.read(obj_in)\n",
    "\n",
    "# Decimate the mesh\n",
    "decimated_mesh = mesh.decimate(target_reduction=1.0 - (9976 / mesh.n_faces))\n",
    "\n",
    "# Save the decimated mesh\n",
    "decimated_mesh.save(obj_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "|------------- obj_path: /home/jakhon37/myprojects/3DFACE/mica_DEV/MICA/dataset/example/nrm_n07.obj\n",
      "\n",
      "len of vertices: 19275\n",
      "len of faces: 16858\n",
      "len of textures: 0\n"
     ]
    }
   ],
   "source": [
    "# CONVERT 3D OBJ TO 3D PLY FORMAT\n",
    "# FILES: .obj, .ply\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def write_ply(filename, vertices, faces=None):\n",
    "    \"\"\"\n",
    "    Write vertices and faces to a PLY file.\n",
    "\n",
    "    :param filename: Name of the PLY file to write to.\n",
    "    :param vertices: List of vertices. Each vertex is a tuple of (x, y, z) coordinates.\n",
    "    :param faces: List of faces. Each face is a tuple of vertex indices. (Optional)\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"ply\\n\")\n",
    "        f.write(\"format ascii 1.0\\n\")\n",
    "        f.write(f\"element vertex {len(vertices)}\\n\")\n",
    "        f.write(\"property float x\\n\")\n",
    "        f.write(\"property float y\\n\")\n",
    "        f.write(\"property float z\\n\")\n",
    "        if faces:\n",
    "            f.write(f\"element face {len(faces)}\\n\")\n",
    "            f.write(\"property list uchar int vertex_index\\n\")\n",
    "        f.write(\"end_header\\n\")\n",
    "\n",
    "        # Write vertices to the file\n",
    "        for vertex in vertices:\n",
    "            f.write(f\"{vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
    "\n",
    "        # If faces are provided, write them to the file\n",
    "        if faces:\n",
    "            for face in faces:\n",
    "                face_str = ' '.join(map(str, [idx[0] for idx in face]))\n",
    "                f.write(f\"{len(face)} {face_str}\\n\")\n",
    "\n",
    "def load_obj(obj_path):\n",
    "    vertices, textures, faces = [], [], []\n",
    "    with open(obj_path, 'r') as fp:\n",
    "        for line in fp:\n",
    "            line_split = line.split()\n",
    "            if not line_split:\n",
    "                continue\n",
    "            elif line_split[0] == 'v':\n",
    "                vertices.append(list(map(float, line_split[1:4])))\n",
    "            elif line_split[0] == 'vt':\n",
    "                textures.append(list(map(float, line_split[1:3])))\n",
    "            elif line_split[0] == 'f':\n",
    "                # Adjusting the vertex indices by subtracting 1\n",
    "                face = [list(map(lambda x: int(x)-1 if x.isdigit() else x, vert.split('/'))) for vert in line_split[1:]]\n",
    "                faces.append(face)\n",
    "    vertices, textures, faces = np.array(vertices), np.array(textures), faces\n",
    "    print('\\nlen of vertices:', len(vertices))\n",
    "    print('len of faces:', len(faces))\n",
    "    print('len of textures:', len(textures))\n",
    "    return vertices, textures, faces\n",
    "\n",
    "\n",
    "root = os.path.abspath(os.getcwd())\n",
    "\n",
    "obj_path = os.path.join(root, \"example/nrm_n07.obj\")\n",
    "objOut = os.path.join(root, \"example/nrm_n07_scaled.ply\")\n",
    "print('\\n\\n|------------- obj_path:', obj_path)\n",
    "# Load the OBJ file\n",
    "vertices, _, faces = load_obj(obj_path)\n",
    "\n",
    "# Convert to PLY format and save\n",
    "write_ply(objOut, vertices, faces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install plotly\n",
    "# VISUALIZE 3D LANDMARKS\n",
    "# FILES: .npy\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_lnd_points_interactive(lndmark):\n",
    "    trace = go.Scatter3d(\n",
    "        x=lndmark[:, 0],\n",
    "        y=lndmark[:, 1],\n",
    "        z=lndmark[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color='red',\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "root = os.path.abspath(os.getcwd())\n",
    "lnd3d = os.path.join(root, \"test_uzip/label/1107M_FC_A-mpie68.npy\")\n",
    "lnd3d = os.path.join(root, \"example/nrm_n-mpie68.npy\")\n",
    "\n",
    "\n",
    "landmarks3d = np.load(lnd3d)\n",
    "plot_lnd_points_interactive(landmarks3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
